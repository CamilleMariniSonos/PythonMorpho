{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing, debugging, profiling\n",
    "\n",
    "Dans cette section, nous allons voir comment tester, débugger et profiler du code Python. \n",
    "\n",
    "Cette section est adapté des [SciPy lecture notes](http://www.scipy-lectures.org/) et d'un [cours](https://paris-swc.github.io/python-testing-debugging-profiling/) de Marcel Stimberg, lui-même adapté d'un cours de [Kathryn Huff](http://katyhuff.github.io/python-testing/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. Testing\n",
    "\n",
    "### 1.1 Assertions\n",
    "\n",
    "Les assertions sont les types de tests les plus simples. Elles permettent de vérifier la cohérence interne d'un code.  \n",
    "En Python, une assertion a le comportement suivant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "assert True == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert True == True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'assertion arrête l'exécution du code si la comparaison est fausse et ne fait rien si la comparaison est juste.  \n",
    "C'est par exemple un outil pratique pour vérifier que l'input d'une fonction est correcte: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mean(num_list):\n",
    "    assert len(num_list) != 0\n",
    "    return sum(num_list) / len(num_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mean([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'avantage des assertions est leur facilité d'utilisation et leur compacité: une ligne de code! Le désavantage des assertions est qu'elles arrêtent l'exécution du code et ne produisent pas de message informatif à l'utilisateur. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Exceptions\n",
    "\n",
    "Vous avez peut être déjà rencontré une exception Python pendant la journée de hier. Une exception est levée quand on rencontre une erreur. On peut aussi lever des exceptions dans notre code. C'est par exemple pratique pour vérifier si l'entrée d'une fonction est de type attendu, si ce n'est pas le cas une exception peut être levée avec un message informatif.  \n",
    "\n",
    "Considérons par exemple la fonction ci-dessous qui \"rescale\" un tableau numpy sur un intervalle donné: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def rescale(data, lower=0.0, upper=1.0):\n",
    "    \"\"\"\n",
    "    (Linearly) rescale the data so that it fits into the given range.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : ndarray\n",
    "        The data to rescale.\n",
    "    lower : number, optional\n",
    "        The lower bound for the data. Defaults to 0.\n",
    "    upper : number, optional\n",
    "        The upper bound for the data. Defaults to 1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    rescaled : ndarray\n",
    "        The data rescaled between ``lower`` and ``upper``.\n",
    "    \"\"\"\n",
    "    data_min = np.min(data)\n",
    "    data_max = np.max(data)\n",
    "    normalized_data = (data - data_min) / (data_max - data_min)\n",
    "    rescaled_data = lower + (upper - lower) * normalized_data\n",
    "    return rescaled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que se passe-t-il si les éléments du tableau d'entrée ont tous la même valeur?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rescale(np.array([0, 0, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour éviter cela, nous allons lever l'exception \"Value Error\" si les éléments du tableau d'entrée ont tous la même valeur. \n",
    "`ValueError` est une des classes d'erreurs intégrée dans Python, qui permet de signaler un argument qui a le bon type mais une valeur inappropriée.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rescale(data, lower=0.0, upper=1.0):\n",
    "    data_min = np.min(data)\n",
    "    data_max = np.max(data)\n",
    "    if not data_max > data_min:\n",
    "        raise ValueError('Cannot rescale data: all values are identical.')\n",
    "    normalized_data = (data - data_min) / (data_max - data_min)\n",
    "    rescaled_data = lower + (upper - lower) * normalized_data\n",
    "    return rescaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rescale(np.array([0, 0, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe un certain nombre d'exceptions prédéfinies dans Python ([voir la liste ici](https://docs.python.org/2/library/exceptions.html)). \n",
    "Vous pouvez aussi définir vos propres exceptions. Elles doivent dériver de la classe `Exception` de Python ([plus de détails ici](https://docs.python.org/2/tutorial/errors.html#tut-userexceptions)).\n",
    "\n",
    "Au lieu d'arrêter l'exécution du code quand une erreur est recontrée, on peut \"attrapper\" l'exception afin de modifier le comportement du code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rescale(data, lower=0.0, upper=1.0):\n",
    "    try:\n",
    "        data_min = np.min(data)\n",
    "    except ValueError:\n",
    "        raise ValueError('Could not calculate the minimum of the input data -- maybe it is empty?')\n",
    "    data_max = np.max(data)\n",
    "    if not data_max > data_min:\n",
    "        raise ValueError('Cannot rescale data: all values are identical.')\n",
    "    normalized_data = (data - data_min) / (data_max - data_min)\n",
    "    rescaled_data = lower + (upper - lower) * normalized_data\n",
    "    return rescaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rescale(np.array([]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rescale(data, lower=0.0, upper=1.0):\n",
    "    try:\n",
    "        data_min = np.min(data)\n",
    "    except ValueError:\n",
    "        return np.array([])\n",
    "    data_max = np.max(data)\n",
    "    if not data_max > data_min:\n",
    "        raise ValueError('Cannot rescale data: all values are identical.')\n",
    "    normalized_data = (data - data_min) / (data_max - data_min)\n",
    "    rescaled_data = lower + (upper - lower) * normalized_data\n",
    "    return rescaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rescale(np.array([]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut aussi gérer plusieurs types d'exception:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rescale(data, lower=0.0, upper=1.0):\n",
    "    try:\n",
    "        data_min = np.min(data)\n",
    "    except ValueError:\n",
    "        return np.array([])\n",
    "    except TypeError:\n",
    "        raise TypeError('Can only rescale numerical data')\n",
    "    data_max = np.max(data)\n",
    "    if not data_max > data_min:\n",
    "        raise ValueError('Cannot rescale data: all values are identical.')\n",
    "    normalized_data = (data - data_min) / (data_max - data_min)\n",
    "    rescaled_data = lower + (upper - lower) * normalized_data\n",
    "    return rescaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rescale(np.array(['chien', 'chat']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quel est l'avantage d'utiliser `try/except` par rapport à une vérification explicite? Comparez ces deux fonctions qui renvoient le minimum et le maximum comme une tuple:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minmax1(data):\n",
    "   try:\n",
    "       data_min = np.min(data)\n",
    "       data_max = np.max(data)\n",
    "   except TypeError:\n",
    "       raise TypeError('Need numerical data.')\n",
    "   return (data_min, data_max)\n",
    "\n",
    "def minmax2(data):\n",
    "   if not isinstance(data, np.ndarray):\n",
    "       raise TypeError('Need numerical data.')\n",
    "   data_min = np.min(data)\n",
    "   data_max = np.max(data)\n",
    "   return (data_min, data_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l = [1, 2, 3]\n",
    "print(minmax1(l))\n",
    "print(minmax2(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Tests unitaires\n",
    "\n",
    "Ce que nous venons de voir nous protége contre certaines erreurs, mais on ne peut pas savoir si notre code marche avant de l'utiliser... Heureusement, on peut écrire des tests unitaires!  \n",
    "En Python, les tests unitaires sont typiquement des fonctions de tests qui appellent les fonctions dans le code de base et font des assertions. Pour faire tourner ces tests, un \"framework\" de test est souvent requit, il permet de collecter tous les tests et de les éxécuter.  \n",
    "Nous allons d'abord écrire des tests pour notre fonction `rescale` et les exécuter individuellement. Nous introduirons ensuite le \"framework\" de test [`pytest`](http://pytest.org/latest/). Python fournit un framework de test intégré [`unittest`](https://docs.python.org/3/library/unittest.html). Toutefois, le framework pytest est plus simple d'utilisation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un test comporte générallement trois parties: **\"Etant donné, quand, alors\"**. Etant donné certaines données, quand telle action est réalisée, alors on attend ce résultat.\n",
    "\n",
    "Note: Numpy fournit quelques fonctions utiles pour effectuer des tests dans le package [numpy.testing.utils](http://docs.scipy.org/doc/numpy-1.10.1/reference/routines.testing.html), comme `assert_allclose`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rescale(data, lower=0.0, upper=1.0):\n",
    "    try:\n",
    "        data_min = np.min(data)\n",
    "    except ValueError:\n",
    "        return np.array([])\n",
    "    except TypeError:\n",
    "        raise TypeError('Can only rescale numerical data')\n",
    "    data_max = np.max(data)\n",
    "    if not data_max > data_min:\n",
    "        raise ValueError('Cannot rescale data: all values are identical.')\n",
    "    normalized_data = (data - data_min) / (data_max - data_min)\n",
    "    rescaled_data = lower + 2 * (upper - lower) * normalized_data\n",
    "    return rescaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.testing.utils import assert_allclose\n",
    "\n",
    "def test_rescale():\n",
    "    data = np.array([1.0, 2.0, 3.0])\n",
    "    rescaled = rescale(data)\n",
    "    expected = np.array([0.0, 0.5, 1.0])\n",
    "    assert_allclose(rescaled, expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_rescale()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests unitaires avec Pytest\n",
    "\n",
    "Pytest permet l'utilisation de la fonction standard `assert` et de ses dérivées ([plus de détails ici]()). \n",
    "\n",
    "Nous allons écrire une suite de tests pour la fonction `preprocess.py` et les exécuter grâce à `pytest`. Voici la fonction `preprocess.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: exo_testing/preprocess.py: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!cat exo_testing/preprocess.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons écrire des tests pour cette fonction dans le fichier `test_whiten.py` (qui utilisent l'assertion `assert_allclose`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile exo_testing/test_whiten.py\n",
    "from numpy.testing.utils import assert_allclose, assert_equal\n",
    "import numpy\n",
    "\n",
    "from preprocess import whiten\n",
    "\n",
    "def test_1d():\n",
    "    test_data = numpy.array([1, 3, 5, 7])\n",
    "    whitened = whiten(test_data)\n",
    "    assert_allclose(whitened.mean(), 0)\n",
    "    assert_allclose(whitened.std(), 1)\n",
    "    assert_allclose(whitened*test_data.std() + test_data.mean(),\n",
    "                    test_data)\n",
    "def test_2d():\n",
    "    test_data = numpy.array([[1, 3, 5, 7],\n",
    "                             [2, 3, 4, 1]])\n",
    "    whitened = whiten(test_data)\n",
    "    assert_allclose(whitened.mean(), 0)\n",
    "    assert_allclose(whitened.std(), 1)\n",
    "    assert_allclose(whitened*test_data.std() + test_data.mean(),\n",
    "                    test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant exécuter `pytest` (commande `py.test`) dans le dossier où se trouvent le fichier de test.\n",
    "\n",
    "Le package pytest collecte les tests (fichiers `test_*`) dans le dossier, les éxécute et fournit un rapport de test.\n",
    "Pour avoir plus/moins de détails sur les tests, on peut utiliser l'option `-v`/`-q` (pour `verbose`/`quiet`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos test ne passent pas, car ils sont en fait trop strict! Le rapport nous informe que:\n",
    "```\n",
    "E       AssertionError: \n",
    "E       Not equal to tolerance rtol=1e-07, atol=0\n",
    "```\n",
    "La tolérance absolue est trop stricte.. On peut ajouter `atol=1e-15` à notre assertion pour résoudre ce problème."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile exo_testing/test_whiten.py\n",
    "from numpy.testing.utils import assert_allclose, assert_equal\n",
    "import numpy\n",
    "\n",
    "from preprocess import whiten\n",
    "\n",
    "def test_1d():\n",
    "    test_data = numpy.array([1, 3, 5, 7])\n",
    "    whitened = whiten(test_data)\n",
    "    assert_allclose(whitened.mean(), 0, atol=1e-15)\n",
    "    assert_allclose(whitened.std(), 1, atol=1e-15)\n",
    "    assert_allclose(whitened*test_data.std() + test_data.mean(),\n",
    "                    test_data)\n",
    "def test_2d():\n",
    "    test_data = numpy.array([[1, 3, 5, 7],\n",
    "                             [2, 3, 4, 1]])\n",
    "    whitened = whiten(test_data)\n",
    "    assert_allclose(whitened.mean(), 0, atol=1e-15)\n",
    "    assert_allclose(whitened.std(), 1, atol=1e-15)\n",
    "    assert_allclose(whitened*test_data.std() + test_data.mean(),\n",
    "                    test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Debugging\n",
    "\n",
    "### 2.1 Post-mortem debugging\n",
    "\n",
    "Python fournit un debugger intégré. Quand une exception est levée, son contexte est enregistré et peut être examiné. Ce type de debugging est dit \"post-mortem\", car on utilise le debugger après le crash du programme, par opposition à exécuter le programme sous le contrôle du debugger.\n",
    "\n",
    "Voici du code avec des erreurs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def find_first(data, element):\n",
    "    \"\"\"\n",
    "    Return the index of the first appearance of ``element`` in\n",
    "    ``data`` (or -1 if ``data`` does not contain ``element``).\n",
    "    \"\"\"\n",
    "    counter = 0\n",
    "    while counter <= len(data):\n",
    "        if data[counter] == element:\n",
    "            return counter\n",
    "        counter += 1\n",
    "    return -1\n",
    "\n",
    "def check_data(target):\n",
    "    test_data = [3, 2, 8, 9, 3, np.nan, 4, 7, 5]\n",
    "    # We look for a zero in the data\n",
    "    index = find_first(test_data, target)\n",
    "    if index != -1:\n",
    "        print('Data until first occurrence of', target, ':', test_data[:index])\n",
    "    else:\n",
    "        print('No occurrence of', target, 'in the data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction `find_first` trouve la première occurence d'un élément dans une liste (ou un tableau) et renvoie son indice. La fonction `check_data` utilise cette fonction pour trouver une certaine valeur dans les data et affiche les données jusqu'à ce point. Elle utilise des fausses données et prend comme argument `target` qui spécifie la valeur à trouver dans les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "check_data(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "check_data(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La commande `%debug` permet dans IPython ou dans le Jupyter Notebook d'accéder à la ligne de l'erreur. Elle lance la console `ipdb` (une interface améliorée du debugger intégré de Python `pdb`). On peut alors  accéder à l'état des variables à cette ligne.  \n",
    "Parfois, les variables qui nous intéressent ne sont pas accéssibles à la ligne où l'exception a été levée. On peut alors se déplacer avec des commandes, telles que:\n",
    "* `up` or `u`   \n",
    "* `down` ou `d  \n",
    "* `next` ou `n`\n",
    "* `step` ou `s`: pour exécuter la ligne et en entrant dans les fonctions appelées dans cette ligne.  \n",
    "* `continue` ou `c`  \n",
    "* `quit` ou `q`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post mortem debugging avec pytest\n",
    "\n",
    "On peut exécuter la suite de tests avec pytest et lui faire ouvrir le debugger quand il y a une erreur:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m====================================== test session starts =======================================\u001b[0m\r\n",
      "platform linux2 -- Python 2.7.6, pytest-2.9.1, py-1.4.31, pluggy-0.3.1\r\n",
      "rootdir: /home/camille/Documents/Cours/PythonMorpho/exo_testing, inifile: \r\n",
      "\u001b[1m\r",
      "collecting 0 items\u001b[0m\u001b[1m\r",
      "collected 0 items \r\n",
      "\u001b[0m\r\n",
      "\u001b[1m\u001b[33m================================== no tests ran in 0.00 seconds ==================================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!cd exo_testing; py.test --pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Debugging pas à pas\n",
    "\n",
    "Regardons les fichiers `historgrams.py` et `do_equalization.py.` `historgrams.py` définit deux fonctions: `plot_histogram` et `equalize`. Le fonction `plot_histogram` dessine l'histogramme des valeurs dans un tabelau et la distribution cumulée de l'histograme. La fonction `equalize` réalise l'algorithme d'équalisation d'histogramme sur un image en niveau de gris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: exo_testing/histograms.py: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!cat exo_testing/histograms.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Cet algorithe permet de transformer l'image pour qu'elle ait un histogramme plat des valeurs d'intensité.\n",
    "\n",
    "Essayons d'éxécuter `do_equalization.py` qui plotte un exemple d'image et son histogramme, l'équalise en utilisant la fonction ci-dessus et plotte le résultat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: File `u'exo_testing/do_equalization.py'` not found.\n"
     ]
    }
   ],
   "source": [
    "%run exo_testing/do_equalization.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ca ne fonctionne pas... Les arguments de `np.interp` semblent ne pas avoir la même longueur. Utilisons le debugger pour examiner ce qui se passe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On dirait que `bins` et `cdf` que l'on donne à `np.interp` ont une longueur de différence. C'est quelque chose qui arrive souvent quand on travaille avec les histogrammes, car on utilise parfois des classes (N valeurs) et parfois des limites de classes (N+1 valeurs).    \n",
    "Essayons de réparer celà en enlevant la dernière valeur de la variable `cdf`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarque sur Jupyter notebook et les fichiers externes:**  \n",
    "Python n'importe un module qu'une seule fois. Du coup, si on édite une fonction dans un fichier, qui a déjà été importée, les changements ne seront pas pris en compte dans le notebook. Ce qu'on peut faire contre cela est de forcer le rechargement de tous les modules pour chaque execution de code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bon, ça ne marche pas toujours, il faut parfois redémarrer le kernel du notebook et ré-exécuter les cellules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run exo_testing/do_equalization.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hum... même si on n'obtient pas de message d'erreur, ça n'a pas l'air de bien marcher. On ne peut pas faire ici du \"post mortem debugging\", nous allons donc exécuter le script sous le contrôle du debugger avec la command:  \n",
    "`%run -d do_equalization.py`.   \n",
    "Hors du notebook, on peut faire appel à:   \n",
    "`python -m pdb script.py`.\n",
    "\n",
    "de même qu'avec `%debug`, on a un prompt, mais on peut maintenant exécuter le script. On peut utiliser les commandes mentionnées plus haut (`n`, `s`, `c`) pour naviguer dans l'exécution du fichier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breakpoint 1 at /home/camille/Documents/Cours/PythonMorpho/do_equalization.py:1\n",
      "NOTE: Enter 'c' at the ipdb>  prompt to continue execution.\n",
      "> \u001b[1;32m/home/camille/Documents/Cours/PythonMorpho/do_equalization.py\u001b[0m(1)\u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31m1\u001b[1;32m---> 1 \u001b[1;33m\u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m      2 \u001b[1;33m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m      3 \u001b[1;33m\u001b[1;32mimport\u001b[0m \u001b[0mhistograms\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> b (histogram:20)\n",
      "***  '(histogram' not found from sys.path\n",
      "ipdb> b histograms.py:20\n",
      "*** Blank or comment\n",
      "ipdb> break histograms:20\n",
      "*** Blank or comment\n",
      "ipdb> break histograms.py:20\n",
      "*** Blank or comment\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "%run -d exo_testing/do_equalization.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Profiling\n",
    "\n",
    "Un petit rappel sur l'approche générale:  \n",
    "1. Vérifier que le code est correct  \n",
    "2. Ecrire des tests pour être sûr que le code sera toujours correct après la phase d'optimisation  \n",
    "3. Mesurer le temps total d'exécution et décider si on a besoin d'optimiser le code.  \n",
    "4. Profiler le code pour décider où faire l'optimisation.  \n",
    "5. Optimiser et réitérer au besoin.\n",
    "\n",
    "<center>\n",
    "<figure>\n",
    "<img src='img/is_it_worth_the_time.png' width=400>\n",
    "<figcaption>Is it worth the time? XKCD comic, licensed CC BY-NC 2.5</figcaption>\n",
    "</figure>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Mesurer le temps total d'exécution  \n",
    "\n",
    "IPython fournit deux commandes:\n",
    "* `%time` mesure le temps total d'exécution de façon simple (comme la commande `time` du Unix shell). On peut donc l'utiliser si la commande/script a un long temps d'exécution.  \n",
    "* `%timeit` répète les mesures plusieurs fois (3 par defaut) et chaque essai exécute la commande N fois (N est choisi pour que l'éxécution dure quelques secondes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.6 s, sys: 24.1 s, total: 38.8 s\n",
      "Wall time: 10 s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "square_ar = np.random.rand(1000, 1000)\n",
    "%time w, v = np.linalg.eig(square_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 3.96 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit square_ar.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Mesurer plus en détails...\n",
    "\n",
    "Les commandes ci-dessus nous aide à décider s'il s'avère utile d'optimiser notre code ou pas, mais elles ne nous donnent pas de renseignements sur quelles parties du code sont à optimiser.  \n",
    "\n",
    "Comme exemple, on va utiliser la classique séquence de Fibonaci, où chaque élément est la somme des deux précédents éléments. On peut écrire cela avec une fonction récurcive (qui va avoir des temps d'exécution dramatiques quand `n` augmente...):  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fibonacci(n):\n",
    "    if n < 2:\n",
    "        return n  # fibonacci(0) == 0, fibonacci(1) == 1\n",
    "    else:\n",
    "        return fibonacci(n - 2) + fibonacci(n - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 62 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time fibonacci(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 3.08 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6765"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time fibonacci(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.18 s, sys: 48 ms, total: 4.23 s\n",
      "Wall time: 4.19 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9227465"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time fibonacci(35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour avoir une idée de ce qu'il se passe, on peut utiliser `%prun` qui fait appel au profiler intégré de Python.  \n",
    "Hors de IPython/notebook, on peut utiliser:  \n",
    "`python python -m cProfile [-o output_file] [-s sort_order] myscript.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: cProfile.py [-o output_file_path] [-s sort] scriptfile [arg] ...\r\n",
      "\r\n",
      "Options:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  -o OUTFILE, --outfile=OUTFILE\r\n",
      "                        Save stats to <outfile>\r\n",
      "  -s SORT, --sort=SORT  Sort order when printing to stdout, based on pstats.Stats class\r\n"
     ]
    }
   ],
   "source": [
    "!python -m cProfile --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    }
   ],
   "source": [
    "%prun fibonacci(35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cela nous renvoie trois types d'information pour chaque fonction appelée pendant l'exécution:\n",
    "* `ncalls`: le nombre de fois que la fonction a été apellée  \n",
    "* `tottime`: le temps total passé dans cette fonction  \n",
    "* `cumtime`: le temps passé dans cette fonction, y compris le temps passé dans les fonctions appelées par cette fonction.  \n",
    "\n",
    "De façon non surprenante, tout le temps est passé dans `fibonacci`, la fonction est appelé un grand nombre de fois!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avez vous une idée pour programmer une fonction `fibonacci` un plus performante?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fib(n):\n",
    "    a,b = 1,1\n",
    "    for i in range(n - 1):\n",
    "        a, b = b, a + b\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour des gros projets, il peut être pratique d'utiliser un outil graphique qui représente les informations de profiling de façon plus ergonomique.  \n",
    "Un de ces outils est `snakeviz`.  \n",
    "Sous IPython/notebook, on tape:\n",
    "<code>\n",
    "%load_ext snakeviz\n",
    "%snakeviz fib(10)\n",
    "</code>\n",
    "Sinon:\n",
    "<code>\n",
    "$ python -m cProfile -o filename myscript.py\n",
    "$ snakeviz filename\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext snakeviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "*** Profile stats marshalled to file u'/tmp/tmpJPO_DZ'. \n"
     ]
    }
   ],
   "source": [
    "%snakeviz fib(35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "*** Profile stats marshalled to file u'/tmp/tmpSEbwKh'. \n"
     ]
    }
   ],
   "source": [
    "%snakeviz fibonacci(35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`%prun` et `snakeviz` nous indiquent les fonctions qui prennent le plus de temps, mais n'aident pas à savoir quel est le potentiel d'optimisation dans les fonctions elle-mêmes.  \n",
    "\n",
    "Un autre type de profiling est le **line-based profiling**, qui mesure le temps d'exécution de chaque ligne dans une ou plusieurs fonctions. Cette fonctionnalité est fournit par le **package `line_profiler`**. \n",
    "Sous IPython/notebook, on tape: \n",
    "<code>\n",
    "%load_ext lprun\n",
    "%lprun -f fonction_name fonction_name(20)\n",
    "</code>\n",
    "Sinon, il faut d'abord annoter la fonction avec le décorateur `@profile`. Ensuite il faut utiliser `kernprof` au lieu de python pour exécuter le script (car `python` ne connait pas `@profile`). Cela va créer un fichier `myscript.py.lprof` qui peut être afficher avec python.\n",
    "<code>\n",
    "$ kernprof -l myscript.py\n",
    "$ python -m line_profiler myscript.py.lprof\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%lprun -f fib fib(35)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
